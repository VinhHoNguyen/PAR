# -*- coding: utf-8 -*-
"""Prompt_Market1501

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mqZGPc3pjXM-NsJD8m6Wnr7I0ipEj3fi
"""

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 1: MOUNT GOOGLE DRIVE                                  â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from google.colab import drive
drive.mount('/content/drive')

import os
CHECKPOINT_DIR = '/content/drive/MyDrive/PromptPAR_checkpoints/Vit_Market1501'
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
os.makedirs('/data/jinjiandong', exist_ok=True)

print(f"âœ… Checkpoint dir: {CHECKPOINT_DIR}")

# Commented out IPython magic to ensure Python compatibility.
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 3: CLONE REPOSITORIES                                  â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os
import shutil

# %cd /content

# Clone OpenPAR
if os.path.exists('/content/OpenPAR'):
    shutil.rmtree('/content/OpenPAR')
! git clone https://github.com/Event-AHU/OpenPAR.git
print("âœ… Cloned OpenPAR")

# Clone Market-1501 Attributes
if os.path.exists('/content/Market-1501_Attribute'):
    shutil.rmtree('/content/Market-1501_Attribute')
!git clone https://github.com/vana77/Market-1501_Attribute.git
print("âœ… Cloned Market-1501_Attribute")

# Verify
!ls -la /content/

# Commented out IPython magic to ensure Python compatibility.
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 4: CÃ€I Äáº¶T DEPENDENCIES                                â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# %cd /content/OpenPAR/PromptPAR

! pip install -q ftfy regex tqdm easydict scipy
!pip install -q git+https://github.com/openai/CLIP.git

print("\nâœ… Dependencies installed")

# Commented out IPython magic to ensure Python compatibility.
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 5: Táº¢I MARKET-1501 DATASET                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os

os.makedirs('/content/data', exist_ok=True)
# %cd /content/data

# Táº£i Market-1501
if not os.path. exists('/content/data/Market-1501-v15.09.15'):
    print("ğŸ”„ Äang táº£i Market-1501...")
    !pip install -q gdown
    !gdown "https://drive.google.com/uc?id=0B8-rUzbwVRk0c054eEozWG9COHM" -O Market-1501-v15.09.15.zip --fuzzy
    !unzip -q Market-1501-v15.09.15.zip
    !rm Market-1501-v15.09.15. zip
    print("âœ… ÄÃ£ táº£i vÃ  giáº£i nÃ©n Market-1501")
else:
    print("âœ… Market-1501 Ä‘Ã£ tá»“n táº¡i")

! ls -la /content/data/Market-1501-v15.09.15/

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 6: Táº¢I VIT PRETRAINED MODEL                            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os

os.makedirs('/data/jinjiandong', exist_ok=True)

vit_path = '/data/jinjiandong/jx_vit_base_p16_224-80ecf9dd.pth'

if not os.path.exists(vit_path):
    print("ğŸ”„ Äang táº£i ViT pretrained...")
    !wget -q https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth -O {vit_path}

if os.path.exists(vit_path):
    size = os.path.getsize(vit_path) / 1024 / 1024
    print(f"âœ… ViT pretrained: {size:.1f} MB")
else:
    print("âŒ Táº£i ViT tháº¥t báº¡i!")

# Commented out IPython magic to ensure Python compatibility.
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 7: Sá»¬A CODE PROMPTPAR Äá»‚ Há»– TRá»¢ MARKET1501             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os
import shutil

# Clone láº¡i Ä‘á»ƒ Ä‘áº£m báº£o code sáº¡ch
# %cd /content
if os.path.exists('/content/OpenPAR'):
    shutil.rmtree('/content/OpenPAR')
! git clone -q https://github.com/Event-AHU/OpenPAR.git
print("âœ… Cloned fresh OpenPAR")

# ========== 1. Sá»¬A clip/model.py ==========
model_py = '/content/OpenPAR/PromptPAR/clip/model.py'

with open(model_py, 'r') as f:
    lines = f.readlines()

new_lines = []
for line in lines:
    # Sá»­a dÃ²ng assert
    if "assert args.dataset in [" in line and "Market1501" not in line:
        new_lines.append("assert args.dataset in ['PA100k', 'RAPV1','RAPV2','PETA','WIDER','PETAzs','RAPzs','UPAR','YCJC','RAPV1Expand','Market1501'], \\\n")
    # Sá»­a dÃ²ng datasets_attrnum
    elif "datasets_attrnum=" in line and "Market1501" not in line:
        new_lines. append("datasets_attrnum={'PA100k':26,'RAPV1':51,'PETA':35,'PETAzs':35,'UPAR':40,'RAPzs':53,'RAPV2':54,'WIDER':14,'RAPV1Expand':51,'Market1501':27}\n")
    else:
        new_lines.append(line)

with open(model_py, 'w') as f:
    f. writelines(new_lines)

print("âœ… ÄÃ£ sá»­a clip/model.py")

# ========== 2. Sá»¬A dataset/AttrDataset.py ==========
attr_py = '/content/OpenPAR/PromptPAR/dataset/AttrDataset.py'

with open(attr_py, 'r') as f:
    content = f.read()

# ThÃªm Market1501
content = content.replace(
    "['PA100k', 'RAPV1','RAPV2','PETA','WIDER','RAPzs','PETAzs','UPAR','YCJC',]",
    "['PA100k', 'RAPV1','RAPV2','PETA','WIDER','RAPzs','PETAzs','UPAR','YCJC','Market1501']"
)

# Sá»­a Ä‘Æ°á»ng dáº«n dataset
content = content. replace(
    "dataset_dir='/data/jinjiandong/datasets/'",
    "dataset_dir='/content/data/'"
)

with open(attr_py, 'w') as f:
    f.write(content)

print("âœ… ÄÃ£ sá»­a dataset/AttrDataset.py")

# ========== VERIFY ==========
print("\nğŸ“‹ Verify model.py:")
with open(model_py, 'r') as f:
    for i, line in enumerate(f, 1):
        if 'datasets_attrnum' in line:
            print(f"   Line {i}: {line.strip()[:70]}...")
            if "'Market1501':27" in line:
                print("   âœ… Market1501 = 27 attributes")
            else:
                print("   âŒ ChÆ°a Ä‘Ãºng!")
            break

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 7 FIX: Sá»¬A TRá»°C TIáº¾P FILE MODEL.PY                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

model_py = '/content/OpenPAR/PromptPAR/clip/model.py'

with open(model_py, 'r') as f:
    content = f.read()

# Hiá»ƒn thá»‹ trÆ°á»›c khi sá»­a
print("ğŸ” TRÆ¯á»šC KHI Sá»¬A:")
for line in content.split('\n')[13:19]:
    print(f"   {line[:80]}")

# Sá»­a táº¥t cáº£ cÃ¡c pattern cÃ³ thá»ƒ
replacements = [
    # Pattern 1
    ("datasets_attrnum={'PA100k':26,'RAPV1':51,'PETA':35,'PETAzs':35,'UPAR':40,'RAPzs':53,'RAPV2':54,'WIDER':14,\"RAPV1Expand\":51}",
     "datasets_attrnum={'PA100k':26,'RAPV1':51,'PETA':35,'PETAzs':35,'UPAR':40,'RAPzs':53,'RAPV2':54,'WIDER':14,'RAPV1Expand':51,'Market1501':27}"),
    # Pattern 2 (náº¿u Ä‘Ã£ cÃ³ Market1501 nhÆ°ng sai sá»‘)
    ("'Market1501':39", "'Market1501':27"),
    # Assert pattern
    ("['PA100k', 'RAPV1','RAPV2','PETA','WIDER','PETAzs','RAPzs','UPAR','YCJC',\"RAPV1Expand\"]",
     "['PA100k', 'RAPV1','RAPV2','PETA','WIDER','PETAzs','RAPzs','UPAR','YCJC','RAPV1Expand','Market1501']"),
]

for old, new in replacements:
    if old in content:
        content = content. replace(old, new)
        print(f"\nâœ… ÄÃ£ thay tháº¿ pattern")

with open(model_py, 'w') as f:
    f.write(content)

# Hiá»ƒn thá»‹ sau khi sá»­a
print("\nğŸ” SAU KHI Sá»¬A:")
with open(model_py, 'r') as f:
    content = f.read()
for line in content.split('\n')[13:19]:
    print(f"   {line[:80]}")

# Verify
if "'Market1501':27" in content:
    print("\nâœ… OK!  Market1501 = 27")
else:
    print("\nâŒ ChÆ°a sá»­a Ä‘Æ°á»£c!  Cáº§n sá»­a thá»§ cÃ´ng")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 8: Táº O DATASET PICKLE CHO MARKET1501                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import scipy.io
import numpy as np
import os
import pickle
import shutil
from easydict import EasyDict

# ========== PATHS ==========
MARKET_DIR = '/content/data/Market-1501-v15.09.15'
ATTR_FILE = '/content/Market-1501_Attribute/market_attribute.mat'
SAVE_DIR = '/content/data/Market1501'

# ========== CHECK FILES ==========
print("ğŸ” Kiá»ƒm tra files...")
if not os.path. exists(MARKET_DIR):
    raise Exception("âŒ ChÆ°a cÃ³ Market-1501!  Cháº¡y Cell 5 trÆ°á»›c.")
if not os.path. exists(ATTR_FILE):
    raise Exception("âŒ ChÆ°a cÃ³ market_attribute.mat! Cháº¡y Cell 3 trÆ°á»›c.")
print("âœ… Files OK")

# ========== LOAD . MAT FILE ==========
mat_data = scipy. io.loadmat(ATTR_FILE)
market_attr = mat_data['market_attribute'][0][0]
train_attr_mat = market_attr['train'][0][0]
test_attr_mat = market_attr['test'][0][0]

# Láº¥y tÃªn attributes Tá»ª FILE . MAT (27 attributes)
attr_names = [name for name in train_attr_mat.dtype. names if name != 'image_index']
print(f"ğŸ“Š Attributes tá»« . mat: {len(attr_names)}")

# ========== GET IMAGE LISTS ==========
train_dir = os.path. join(MARKET_DIR, 'bounding_box_train')
test_dir = os.path. join(MARKET_DIR, 'bounding_box_test')

train_images = sorted([f for f in os. listdir(train_dir)
                      if f.endswith('.jpg') and not f.startswith('-1')])
test_images = sorted([f for f in os. listdir(test_dir)
                     if f.endswith('.jpg') and not f.startswith('-1') and not f.startswith('0000')])

print(f"ğŸ–¼ï¸ Train images: {len(train_images)}")
print(f"ğŸ–¼ï¸ Test images: {len(test_images)}")

# ========== CREATE DATASET ==========
image_name_list = []
label_list = []
train_idx = []
test_idx = []
idx = 0

# Process TRAINING images
print("\nğŸ”„ Processing training images...")
for img_name in train_images:
    try:
        identity_id = int(img_name.split('_')[0])
        if 1 <= identity_id <= 751:
            image_name_list.append(os.path.join('bounding_box_train', img_name))
            attr_idx = identity_id - 1
            label = [int(train_attr_mat[attr_name][0][attr_idx]) - 1
                     if int(train_attr_mat[attr_name][0][attr_idx]) > 0 else 0
                     for attr_name in attr_names]
            label_list.append(label)
            train_idx.append(idx)
            idx += 1
    except:
        continue
print(f"   âœ… Train samples: {len(train_idx)}")

# Process TEST images
print("ğŸ”„ Processing test images...")
for img_name in test_images:
    try:
        identity_id = int(img_name.split('_')[0])
        if 1 <= identity_id <= 750:
            image_name_list.append(os.path. join('bounding_box_test', img_name))
            attr_idx = identity_id - 1
            label = [int(test_attr_mat[attr_name][0][attr_idx]) - 1
                     if int(test_attr_mat[attr_name][0][attr_idx]) > 0 else 0
                     for attr_name in attr_names]
            label_list. append(label)
            test_idx. append(idx)
            idx += 1
    except:
        continue
print(f"   âœ… Test samples: {len(test_idx)}")

# ========== CREATE EASYDICT ==========
dataset = EasyDict()
dataset.description = 'market1501'
dataset.root = SAVE_DIR
dataset. image_name = image_name_list
dataset.label = np.array(label_list, dtype=np.float32)
dataset. attributes = attr_names  # 27 attributes tá»« .mat

dataset.partition = EasyDict()
dataset.partition. train = np.array(train_idx, dtype=np.int64)
dataset.partition.val = np.array([], dtype=np.int64)
dataset. partition.test = np.array(test_idx, dtype=np.int64)
dataset.partition.trainval = np.array(train_idx, dtype=np.int64)

# Weights
train_labels = dataset.label[dataset. partition.train]
label_ratio = np.clip(np.mean(train_labels, axis=0), 0.01, 0.99)
dataset.weight_train = np.exp(-label_ratio). astype(np. float32)
dataset.weight_trainval = dataset.weight_train. copy()

# ========== SAVE ==========
if os. path.exists(SAVE_DIR):
    shutil.rmtree(SAVE_DIR)
os.makedirs(SAVE_DIR)

pkl_path = os.path. join(SAVE_DIR, 'pad. pkl')
with open(pkl_path, 'wb') as f:
    pickle.dump(dataset, f)
print(f"\nâœ… Saved: {pkl_path}")

os.symlink(train_dir, os. path.join(SAVE_DIR, 'bounding_box_train'))
os.symlink(test_dir, os. path.join(SAVE_DIR, 'bounding_box_test'))
print("âœ… Created symlinks")

# ========== VERIFY ==========
print("\n" + "=" * 60)
print("ğŸ“‹ DATASET SUMMARY")
print("=" * 60)
print(f"âœ… Labels shape: {dataset. label.shape}")
print(f"âœ… Attributes: {len(dataset. attributes)}")
print(f"âœ… Train: {len(dataset.partition. train)}")
print(f"âœ… Test: {len(dataset.partition. test)}")

# Verify file exists
if os.path.exists(pkl_path):
    size = os.path. getsize(pkl_path) / 1024 / 1024
    print(f"âœ… pad.pkl: {size:.2f} MB")

if len(dataset.attributes) == dataset.label.shape[1]:
    print(f"\nğŸ‰ OK!  attributes = labels = {len(dataset.attributes)}")
else:
    print(f"\nâŒ MISMATCH!")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  Táº O PAD.PKL                                                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import scipy.io
import numpy as np
import os
import pickle
import shutil
from easydict import EasyDict

# ========== PATHS ==========
MARKET_DIR = '/content/data/Market-1501-v15.09.15'
ATTR_FILE = '/content/Market-1501_Attribute/market_attribute.mat'
SAVE_DIR = '/content/data/Market1501'

# XÃ³a vÃ  táº¡o láº¡i thÆ° má»¥c
if os.path.exists(SAVE_DIR):
    shutil.rmtree(SAVE_DIR)
os.makedirs(SAVE_DIR)

# ========== LOAD . MAT FILE ==========
mat_data = scipy.io. loadmat(ATTR_FILE)
market_attr = mat_data['market_attribute'][0][0]
train_attr_mat = market_attr['train'][0][0]
test_attr_mat = market_attr['test'][0][0]

# Láº¥y 27 attributes tá»« . mat
attr_names = [name for name in train_attr_mat.dtype.names if name != 'image_index']
print(f"ğŸ“Š Attributes: {len(attr_names)}")

# ========== GET IMAGE LISTS ==========
train_dir = os.path. join(MARKET_DIR, 'bounding_box_train')
test_dir = os.path. join(MARKET_DIR, 'bounding_box_test')

train_images = sorted([f for f in os.listdir(train_dir)
                      if f.endswith('.jpg') and not f.startswith('-1')])
test_images = sorted([f for f in os. listdir(test_dir)
                     if f.endswith('.jpg') and not f.startswith('-1') and not f.startswith('0000')])

print(f"ğŸ–¼ï¸ Train: {len(train_images)}, Test: {len(test_images)}")

# ========== CREATE DATASET ==========
image_name_list = []
label_list = []
train_idx = []
test_idx = []
idx = 0

# TRAINING
for img_name in train_images:
    try:
        identity_id = int(img_name.split('_')[0])
        if 1 <= identity_id <= 751:
            image_name_list.append(os.path.join('bounding_box_train', img_name))
            attr_idx = identity_id - 1
            label = [max(0, int(train_attr_mat[attr_name][0][attr_idx]) - 1) for attr_name in attr_names]
            label_list.append(label)
            train_idx.append(idx)
            idx += 1
    except:
        continue

# TEST
for img_name in test_images:
    try:
        identity_id = int(img_name.split('_')[0])
        if 1 <= identity_id <= 750:
            image_name_list.append(os.path. join('bounding_box_test', img_name))
            attr_idx = identity_id - 1
            label = [max(0, int(test_attr_mat[attr_name][0][attr_idx]) - 1) for attr_name in attr_names]
            label_list.append(label)
            test_idx.append(idx)
            idx += 1
    except:
        continue

print(f"âœ… Train: {len(train_idx)}, Test: {len(test_idx)}")

# ========== CREATE EASYDICT ==========
dataset = EasyDict()
dataset.description = 'market1501'
dataset.root = SAVE_DIR
dataset. image_name = image_name_list
dataset.label = np.array(label_list, dtype=np.float32)
dataset. attributes = attr_names

dataset.partition = EasyDict()
dataset.partition.train = np.array(train_idx, dtype=np.int64)
dataset.partition.val = np.array([], dtype=np.int64)
dataset. partition.test = np.array(test_idx, dtype=np.int64)
dataset.partition.trainval = np.array(train_idx, dtype=np.int64)

# Weights
train_labels = dataset.label[dataset.partition.train]
label_ratio = np.clip(np.mean(train_labels, axis=0), 0.01, 0.99)
dataset.weight_train = np.exp(-label_ratio). astype(np.float32)
dataset.weight_trainval = dataset.weight_train. copy()

# ========== SAVE PICKLE ==========
pkl_path = os. path.join(SAVE_DIR, 'pad.pkl')
with open(pkl_path, 'wb') as f:
    pickle.dump(dataset, f)

# ========== CREATE SYMLINKS ==========
os.symlink(train_dir, os. path.join(SAVE_DIR, 'bounding_box_train'))
os.symlink(test_dir, os. path.join(SAVE_DIR, 'bounding_box_test'))

# ========== VERIFY ==========
print("\n" + "=" * 60)
if os.path.exists(pkl_path):
    size = os.path. getsize(pkl_path) / 1024 / 1024
    print(f"âœ… CREATED: {pkl_path} ({size:.2f} MB)")
    print(f"âœ… Labels: {dataset.label.shape}")
    print(f"âœ… Attributes: {len(dataset.attributes)}")
    print(f"\nğŸ‰ THÃ€NH CÃ”NG! Cháº¡y Cell 10 Ä‘á»ƒ training")
else:
    print("âŒ THáº¤T Báº I!")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 9: TRAINING CONFIGURATION                              â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ==================== THAY Äá»”I á» ÄÃ‚Y ====================
EPOCHS = 30              # Test: 3 | Cháº¡y tháº­t: 30
BATCH_SIZE = 16
SAVE_FREQ = 5           # LÆ°u checkpoint má»—i N epoch
# ========================================================

print("=" * 60)
print("ğŸ“‹ TRAINING CONFIGURATION")
print("=" * 60)
print(f"  Epochs:         {EPOCHS}")
print(f"  Batch size:     {BATCH_SIZE}")
print(f"  Save frequency: Every {SAVE_FREQ} epoch(s)")
print(f"  Checkpoint:     /content/drive/MyDrive/PromptPAR_checkpoints/Vit_Market1501")
print("=" * 60)

# Commented out IPython magic to ensure Python compatibility.
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 10: TRAINING (LÆ¯U THáº²NG VÃ€O DRIVE)                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# %cd /content/OpenPAR/PromptPAR

import os

# Táº¡o thÆ° má»¥c trong Drive
CHECKPOINT_DIR = "/content/drive/MyDrive/PromptPAR_checkpoints/Vit_Market1501"
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

# Táº¡o symlink tá»« local output sang Drive
LOCAL_OUTPUT = "/content/OpenPAR/PromptPAR/exp_result"
if os.path.exists(LOCAL_OUTPUT):
    if os.path.islink(LOCAL_OUTPUT):
        os.unlink(LOCAL_OUTPUT) # Remove symbolic link directly
    elif os.path.isdir(LOCAL_OUTPUT):
        import shutil
        shutil.rmtree(LOCAL_OUTPUT) # Remove directory
os.symlink(CHECKPOINT_DIR, LOCAL_OUTPUT)

print(f" Symlink: {LOCAL_OUTPUT} â†’ {CHECKPOINT_DIR}")
print(" Báº¯t Ä‘áº§u training.. .\n")

!  python train.py Market1501 \
    --batchsize 16 \
    --epoch 30 \
    --height 224 \
    --width 224 \
    --lr 8e-3 \
    --weight_decay 1e-4 \
    --clip_lr 4e-3 \
    --clip_weight_decay 1e-4 \
    --text_prompt 3 \
    --vis_prompt 50 \
    --vis_depth 24 \
    --div_num 4 \
    --overlap_row 2 \
    --mm_layers 1 \
    --smooth_param 0.1 \
    --ag_threshold 0.5 \
    --train_split trainval \
    --valid_split test \
    --save_freq 5 \
    --use_div \
    --use_textprompt \
    --use_mm_former \
    --use_GL \
    --dir $CHECKPOINT_DIR

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 11: KIá»‚M TRA CHECKPOINT                                â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os
import glob

CHECKPOINT_DIR = "/content/drive/MyDrive/PromptPAR_checkpoints/Vit_Market1501"

print("ğŸ“ Checkpoints Ä‘Ã£ lÆ°u:")
print("=" * 60)

# TÃ¬m trong Drive
checkpoints = glob.glob(f'{CHECKPOINT_DIR}/**/*.pth', recursive=True)
checkpoints += glob.glob(f'{CHECKPOINT_DIR}/*. pth')

# TÃ¬m trong local
local_ckpts = glob.glob('/content/OpenPAR/PromptPAR/**/*.pth', recursive=True)

if checkpoints:
    print(f"âœ… Google Drive ({len(checkpoints)} files):")
    for ckpt in checkpoints:
        size = os.path. getsize(ckpt) / 1024 / 1024
        print(f"   ğŸ“¦ {os.path.basename(ckpt)} ({size:.1f} MB)")

if local_ckpts:
    print(f"\nğŸ“ Local ({len(local_ckpts)} files):")
    for ckpt in local_ckpts:
        size = os. path.getsize(ckpt) / 1024 / 1024
        print(f"   ğŸ“¦ {os.path.basename(ckpt)} ({size:.1f} MB)")

    # Copy to Drive
    import shutil
    print("\nğŸ”„ Copying to Drive...")
    for ckpt in local_ckpts:
        dest = os.path.join(CHECKPOINT_DIR, os.path.basename(ckpt))
        shutil. copy(ckpt, dest)
        print(f"   âœ… {os.path.basename(ckpt)}")

if not checkpoints and not local_ckpts:
    print("âŒ ChÆ°a cÃ³ checkpoint nÃ o!")

# Commented out IPython magic to ensure Python compatibility.
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 12: RESUME TRAINING (Náº¾U Bá»Š NGáº®T)                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# âš ï¸ UNCOMMENT VÃ€ CHáº Y Náº¾U COLAB Bá»Š NGáº®T
# TrÆ°á»›c tiÃªn cháº¡y Cell 1, 7, rá»“i cháº¡y cell nÃ y


# %cd /content/OpenPAR/PromptPAR

import os
CHECKPOINT_DIR = "/content/drive/MyDrive/PromptPAR_checkpoints/Vit_Market1501"

! python train.py Market1501 \
    --batchsize 16 \
    --epoch 30 \
    --height 224 \
    --width 224 \
    --lr 8e-3 \
    --weight_decay 1e-4 \
    --clip_lr 4e-3 \
    --text_prompt 3 \
    --vis_prompt 50 \
    --vis_depth 24 \
    --div_num 4 \
    --save_freq 5 \
    --use_div \
    --use_textprompt \
    --use_mm_former \
    --checkpoint \
    --dir $CHECKPOINT_DIR

print("ğŸ’¡ Uncomment code trÃªn vÃ  cháº¡y náº¿u cáº§n resume training")