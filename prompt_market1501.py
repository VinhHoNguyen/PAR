# -*- coding: utf-8 -*-
"""Prompt_Market1501

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mqZGPc3pjXM-NsJD8m6Wnr7I0ipEj3fi
"""

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 1: MOUNT GOOGLE DRIVE                                  â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from google.colab import drive
drive.mount('/content/drive')

import os
CHECKPOINT_DIR = '/content/drive/MyDrive/PromptPAR_checkpoints/Vit_Market1501'
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
os.makedirs('/data/jinjiandong', exist_ok=True)

print(f" Checkpoint dir: {CHECKPOINT_DIR}")

# Commented out IPython magic to ensure Python compatibility.
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 3: CLONE REPOSITORIES                                  â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os
import shutil

# %cd /content

# Clone OpenPAR
if os.path.exists('/content/OpenPAR'):
    shutil.rmtree('/content/OpenPAR')
! git clone https://github.com/Event-AHU/OpenPAR.git
print(" Cloned OpenPAR")

# Clone Market-1501 Attributes
if os.path.exists('/content/Market-1501_Attribute'):
    shutil.rmtree('/content/Market-1501_Attribute')
!git clone https://github.com/vana77/Market-1501_Attribute.git
print(" Cloned Market-1501_Attribute")

# Verify
!ls -la /content/

# Commented out IPython magic to ensure Python compatibility.
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 4: CÃ€I Äáº¶T DEPENDENCIES                                â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# %cd /content/OpenPAR/PromptPAR

! pip install -q ftfy regex tqdm easydict scipy
!pip install -q git+https://github.com/openai/CLIP.git

print("\n Dependencies installed")

# Commented out IPython magic to ensure Python compatibility.
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 5: Táº¢I MARKET-1501 DATASET                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os

os.makedirs('/content/data', exist_ok=True)
# %cd /content/data

# Táº£i Market-1501
if not os.path. exists('/content/data/Market-1501-v15.09.15'):
    print(" Äang táº£i Market-1501...")
    !pip install -q gdown
    !gdown "https://drive.google.com/uc?id=0B8-rUzbwVRk0c054eEozWG9COHM" -O Market-1501-v15.09.15.zip --fuzzy
    !unzip -q Market-1501-v15.09.15.zip
    !rm Market-1501-v15.09.15. zip
    print(" ÄÃ£ táº£i vÃ  giáº£i nÃ©n Market-1501")
else:
    print(" Market-1501 Ä‘Ã£ tá»“n táº¡i")

! ls -la /content/data/Market-1501-v15.09.15/

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 6: Táº¢I VIT PRETRAINED MODEL                            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os

os.makedirs('/data/jinjiandong', exist_ok=True)

vit_path = '/data/jinjiandong/jx_vit_base_p16_224-80ecf9dd.pth'

if not os.path.exists(vit_path):
    print("ğŸ”„ Äang táº£i ViT pretrained...")
    !wget -q https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth -O {vit_path}

if os.path.exists(vit_path):
    size = os.path.getsize(vit_path) / 1024 / 1024
    print(f" ViT pretrained: {size:.1f} MB")
else:
    print(" Táº£i ViT tháº¥t báº¡i!")

# Commented out IPython magic to ensure Python compatibility.
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 7: Sá»¬A CODE PROMPTPAR Äá»‚ Há»– TRá»¢ MARKET1501             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os
import shutil

# Clone láº¡i Ä‘á»ƒ Ä‘áº£m báº£o code sáº¡ch
# %cd /content
if os.path.exists('/content/OpenPAR'):
    shutil.rmtree('/content/OpenPAR')
! git clone -q https://github.com/Event-AHU/OpenPAR.git
print("Cloned fresh OpenPAR")

# ========== 1. Sá»¬A clip/model.py ==========
model_py = '/content/OpenPAR/PromptPAR/clip/model.py'

with open(model_py, 'r') as f:
    lines = f.readlines()

new_lines = []
for line in lines:
    # Sá»­a dÃ²ng assert
    if "assert args.dataset in [" in line and "Market1501" not in line:
        new_lines.append("assert args.dataset in ['PA100k', 'RAPV1','RAPV2','PETA','WIDER','PETAzs','RAPzs','UPAR','YCJC','RAPV1Expand','Market1501'], \\\n")
    # Sá»­a dÃ²ng datasets_attrnum
    elif "datasets_attrnum=" in line and "Market1501" not in line:
        new_lines. append("datasets_attrnum={'PA100k':26,'RAPV1':51,'PETA':35,'PETAzs':35,'UPAR':40,'RAPzs':53,'RAPV2':54,'WIDER':14,'RAPV1Expand':51,'Market1501':27}\n")
    else:
        new_lines.append(line)

with open(model_py, 'w') as f:
    f. writelines(new_lines)

print(" ÄÃ£ sá»­a clip/model.py")

# ========== 2. Sá»¬A dataset/AttrDataset.py ==========
attr_py = '/content/OpenPAR/PromptPAR/dataset/AttrDataset.py'

with open(attr_py, 'r') as f:
    content = f.read()

# ThÃªm Market1501
content = content.replace(
    "['PA100k', 'RAPV1','RAPV2','PETA','WIDER','RAPzs','PETAzs','UPAR','YCJC',]",
    "['PA100k', 'RAPV1','RAPV2','PETA','WIDER','RAPzs','PETAzs','UPAR','YCJC','Market1501']"
)

# Sá»­a Ä‘Æ°á»ng dáº«n dataset
content = content. replace(
    "dataset_dir='/data/jinjiandong/datasets/'",
    "dataset_dir='/content/data/'"
)

with open(attr_py, 'w') as f:
    f.write(content)

print(" ÄÃ£ sá»­a dataset/AttrDataset.py")

# ========== VERIFY ==========
print("\n Verify model.py:")
with open(model_py, 'r') as f:
    for i, line in enumerate(f, 1):
        if 'datasets_attrnum' in line:
            print(f"   Line {i}: {line.strip()[:70]}...")
            if "'Market1501':27" in line:
                print("    Market1501 = 27 attributes")
            else:
                print("    ChÆ°a Ä‘Ãºng!")
            break

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 7 FIX: Sá»¬A TRá»°C TIáº¾P FILE MODEL.PY                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

model_py = '/content/OpenPAR/PromptPAR/clip/model.py'

with open(model_py, 'r') as f:
    content = f.read()

# Hiá»ƒn thá»‹ trÆ°á»›c khi sá»­a
print(" TRÆ¯á»šC KHI Sá»¬A:")
for line in content.split('\n')[13:19]:
    print(f"   {line[:80]}")

# Sá»­a táº¥t cáº£ cÃ¡c pattern cÃ³ thá»ƒ
replacements = [
    # Pattern 1
    ("datasets_attrnum={'PA100k':26,'RAPV1':51,'PETA':35,'PETAzs':35,'UPAR':40,'RAPzs':53,'RAPV2':54,'WIDER':14,\"RAPV1Expand\":51}",
     "datasets_attrnum={'PA100k':26,'RAPV1':51,'PETA':35,'PETAzs':35,'UPAR':40,'RAPzs':53,'RAPV2':54,'WIDER':14,'RAPV1Expand':51,'Market1501':27}"),
    # Pattern 2 (náº¿u Ä‘Ã£ cÃ³ Market1501 nhÆ°ng sai sá»‘)
    ("'Market1501':39", "'Market1501':27"),
    # Assert pattern
    ("['PA100k', 'RAPV1','RAPV2','PETA','WIDER','PETAzs','RAPzs','UPAR','YCJC',\"RAPV1Expand\"]",
     "['PA100k', 'RAPV1','RAPV2','PETA','WIDER','PETAzs','RAPzs','UPAR','YCJC','RAPV1Expand','Market1501']"),
]

for old, new in replacements:
    if old in content:
        content = content. replace(old, new)
        print(f"\n ÄÃ£ thay tháº¿ pattern")

with open(model_py, 'w') as f:
    f.write(content)

# Hiá»ƒn thá»‹ sau khi sá»­a
print("\n SAU KHI Sá»¬A:")
with open(model_py, 'r') as f:
    content = f.read()
for line in content.split('\n')[13:19]:
    print(f"   {line[:80]}")

# Verify
if "'Market1501':27" in content:
    print("\n OK!  Market1501 = 27")
else:
    print("\n ChÆ°a sá»­a Ä‘Æ°á»£c!  Cáº§n sá»­a thá»§ cÃ´ng")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 8: Táº O DATASET PICKLE CHO MARKET1501                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import scipy.io
import numpy as np
import os
import pickle
import shutil
from easydict import EasyDict

# ========== PATHS ==========
MARKET_DIR = '/content/data/Market-1501-v15.09.15'
ATTR_FILE = '/content/Market-1501_Attribute/market_attribute.mat'
SAVE_DIR = '/content/data/Market1501'

# ========== CHECK FILES ==========
print("Kiá»ƒm tra files...")
if not os.path. exists(MARKET_DIR):
    raise Exception("ChÆ°a cÃ³ Market-1501!  Cháº¡y Cell 5 trÆ°á»›c.")
if not os.path. exists(ATTR_FILE):
    raise Exception("ChÆ°a cÃ³ market_attribute.mat! Cháº¡y Cell 3 trÆ°á»›c.")
print("Files OK")

# ========== LOAD . MAT FILE ==========
mat_data = scipy. io.loadmat(ATTR_FILE)
market_attr = mat_data['market_attribute'][0][0]
train_attr_mat = market_attr['train'][0][0]
test_attr_mat = market_attr['test'][0][0]

# Láº¥y tÃªn attributes Tá»ª FILE . MAT (27 attributes)
attr_names = [name for name in train_attr_mat.dtype. names if name != 'image_index']
print(f"ğŸ“Š Attributes tá»« . mat: {len(attr_names)}")

# ========== GET IMAGE LISTS ==========
train_dir = os.path. join(MARKET_DIR, 'bounding_box_train')
test_dir = os.path. join(MARKET_DIR, 'bounding_box_test')

train_images = sorted([f for f in os. listdir(train_dir)
                      if f.endswith('.jpg') and not f.startswith('-1')])
test_images = sorted([f for f in os. listdir(test_dir)
                     if f.endswith('.jpg') and not f.startswith('-1') and not f.startswith('0000')])

print(f"Train images: {len(train_images)}")
print(f"Test images: {len(test_images)}")

# ========== CREATE DATASET ==========
image_name_list = []
label_list = []
train_idx = []
test_idx = []
idx = 0

# Process TRAINING images
print("\n Processing training images...")
for img_name in train_images:
    try:
        identity_id = int(img_name.split('_')[0])
        if 1 <= identity_id <= 751:
            image_name_list.append(os.path.join('bounding_box_train', img_name))
            attr_idx = identity_id - 1
            label = [int(train_attr_mat[attr_name][0][attr_idx]) - 1
                     if int(train_attr_mat[attr_name][0][attr_idx]) > 0 else 0
                     for attr_name in attr_names]
            label_list.append(label)
            train_idx.append(idx)
            idx += 1
    except:
        continue
print(f" Train samples: {len(train_idx)}")

# Process TEST images
print(" Processing test images...")
for img_name in test_images:
    try:
        identity_id = int(img_name.split('_')[0])
        if 1 <= identity_id <= 750:
            image_name_list.append(os.path. join('bounding_box_test', img_name))
            attr_idx = identity_id - 1
            label = [int(test_attr_mat[attr_name][0][attr_idx]) - 1
                     if int(test_attr_mat[attr_name][0][attr_idx]) > 0 else 0
                     for attr_name in attr_names]
            label_list. append(label)
            test_idx. append(idx)
            idx += 1
    except:
        continue
print(f" Test samples: {len(test_idx)}")

# ========== CREATE EASYDICT ==========
dataset = EasyDict()
dataset.description = 'market1501'
dataset.root = SAVE_DIR
dataset. image_name = image_name_list
dataset.label = np.array(label_list, dtype=np.float32)
dataset. attributes = attr_names  # 27 attributes tá»« .mat

dataset.partition = EasyDict()
dataset.partition. train = np.array(train_idx, dtype=np.int64)
dataset.partition.val = np.array([], dtype=np.int64)
dataset. partition.test = np.array(test_idx, dtype=np.int64)
dataset.partition.trainval = np.array(train_idx, dtype=np.int64)

# Weights
train_labels = dataset.label[dataset. partition.train]
label_ratio = np.clip(np.mean(train_labels, axis=0), 0.01, 0.99)
dataset.weight_train = np.exp(-label_ratio). astype(np. float32)
dataset.weight_trainval = dataset.weight_train. copy()

# ========== SAVE ==========
if os. path.exists(SAVE_DIR):
    shutil.rmtree(SAVE_DIR)
os.makedirs(SAVE_DIR)

pkl_path = os.path. join(SAVE_DIR, 'pad. pkl')
with open(pkl_path, 'wb') as f:
    pickle.dump(dataset, f)
print(f"\n Saved: {pkl_path}")

os.symlink(train_dir, os. path.join(SAVE_DIR, 'bounding_box_train'))
os.symlink(test_dir, os. path.join(SAVE_DIR, 'bounding_box_test'))
print("Created symlinks")

# ========== VERIFY ==========
print("\n" + "=" * 60)
print(" DATASET SUMMARY")
print("=" * 60)
print(f"Labels shape: {dataset. label.shape}")
print(f"Attributes: {len(dataset. attributes)}")
print(f"Train: {len(dataset.partition. train)}")
print(f"Test: {len(dataset.partition. test)}")

# Verify file exists
if os.path.exists(pkl_path):
    size = os.path. getsize(pkl_path) / 1024 / 1024
    print(f"pad.pkl: {size:.2f} MB")

if len(dataset.attributes) == dataset.label.shape[1]:
    print(f"\n OK!  attributes = labels = {len(dataset.attributes)}")
else:
    print(f"\n MISMATCH!")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  Táº O PAD.PKL                                                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import scipy.io
import numpy as np
import os
import pickle
import shutil
from easydict import EasyDict

# ========== PATHS ==========
MARKET_DIR = '/content/data/Market-1501-v15.09.15'
ATTR_FILE = '/content/Market-1501_Attribute/market_attribute.mat'
SAVE_DIR = '/content/data/Market1501'

# XÃ³a vÃ  táº¡o láº¡i thÆ° má»¥c
if os.path.exists(SAVE_DIR):
    shutil.rmtree(SAVE_DIR)
os.makedirs(SAVE_DIR)

# ========== LOAD . MAT FILE ==========
mat_data = scipy.io. loadmat(ATTR_FILE)
market_attr = mat_data['market_attribute'][0][0]
train_attr_mat = market_attr['train'][0][0]
test_attr_mat = market_attr['test'][0][0]

# Láº¥y 27 attributes tá»« . mat
attr_names = [name for name in train_attr_mat.dtype.names if name != 'image_index']
print(f"Attributes: {len(attr_names)}")

# ========== GET IMAGE LISTS ==========
train_dir = os.path. join(MARKET_DIR, 'bounding_box_train')
test_dir = os.path. join(MARKET_DIR, 'bounding_box_test')

train_images = sorted([f for f in os.listdir(train_dir)
                      if f.endswith('.jpg') and not f.startswith('-1')])
test_images = sorted([f for f in os. listdir(test_dir)
                     if f.endswith('.jpg') and not f.startswith('-1') and not f.startswith('0000')])

print(f"Train: {len(train_images)}, Test: {len(test_images)}")

# ========== CREATE DATASET ==========
image_name_list = []
label_list = []
train_idx = []
test_idx = []
idx = 0

# TRAINING
for img_name in train_images:
    try:
        identity_id = int(img_name.split('_')[0])
        if 1 <= identity_id <= 751:
            image_name_list.append(os.path.join('bounding_box_train', img_name))
            attr_idx = identity_id - 1
            label = [max(0, int(train_attr_mat[attr_name][0][attr_idx]) - 1) for attr_name in attr_names]
            label_list.append(label)
            train_idx.append(idx)
            idx += 1
    except:
        continue

# TEST
for img_name in test_images:
    try:
        identity_id = int(img_name.split('_')[0])
        if 1 <= identity_id <= 750:
            image_name_list.append(os.path. join('bounding_box_test', img_name))
            attr_idx = identity_id - 1
            label = [max(0, int(test_attr_mat[attr_name][0][attr_idx]) - 1) for attr_name in attr_names]
            label_list.append(label)
            test_idx.append(idx)
            idx += 1
    except:
        continue

print(f" Train: {len(train_idx)}, Test: {len(test_idx)}")

# ========== CREATE EASYDICT ==========
dataset = EasyDict()
dataset.description = 'market1501'
dataset.root = SAVE_DIR
dataset. image_name = image_name_list
dataset.label = np.array(label_list, dtype=np.float32)
dataset. attributes = attr_names

dataset.partition = EasyDict()
dataset.partition.train = np.array(train_idx, dtype=np.int64)
dataset.partition.val = np.array([], dtype=np.int64)
dataset. partition.test = np.array(test_idx, dtype=np.int64)
dataset.partition.trainval = np.array(train_idx, dtype=np.int64)

# Weights
train_labels = dataset.label[dataset.partition.train]
label_ratio = np.clip(np.mean(train_labels, axis=0), 0.01, 0.99)
dataset.weight_train = np.exp(-label_ratio). astype(np.float32)
dataset.weight_trainval = dataset.weight_train. copy()

# ========== SAVE PICKLE ==========
pkl_path = os. path.join(SAVE_DIR, 'pad.pkl')
with open(pkl_path, 'wb') as f:
    pickle.dump(dataset, f)

# ========== CREATE SYMLINKS ==========
os.symlink(train_dir, os. path.join(SAVE_DIR, 'bounding_box_train'))
os.symlink(test_dir, os. path.join(SAVE_DIR, 'bounding_box_test'))

# ========== VERIFY ==========
print("\n" + "=" * 60)
if os.path.exists(pkl_path):
    size = os.path. getsize(pkl_path) / 1024 / 1024
    print(f"CREATED: {pkl_path} ({size:.2f} MB)")
    print(f"Labels: {dataset.label.shape}")
    print(f"Attributes: {len(dataset.attributes)}")
    print(f"\n THÃ€NH CÃ”NG! Cháº¡y Cell 10 Ä‘á»ƒ training")
else:
    print("THáº¤T Báº I!")

# --- PATCH: lÆ°u checkpoint tháº³ng vÃ o Drive, khÃ´ng dÃ¹ng timestamp ---

train_py = "/content/OpenPAR/PromptPAR/train.py"

with open(train_py, "r") as f:
    code = f.read()

# Block cÅ© dÃ¹ng timestamp
old = """start_time=time_str()
    print(f'start_time is {start_time}')
    log_dir = os.path.join('logs', args.dataset)
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    log_dir = os.path.join(log_dir, start_time)
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)"""

# Block má»›i: láº¥y Ä‘Æ°á»ng dáº«n tá»« --dir
new = """log_dir = args.dir
    os.makedirs(log_dir, exist_ok=True)"""

# Thay tháº¿
code = code.replace(old, new)

with open(train_py, "w") as f:
    f.write(code)

print("train.py Ä‘Ã£ Ä‘Æ°á»£c chá»‰nh Ä‘á»ƒ lÆ°u vÃ o Drive")

# PATCH: thÃªm resume logic vÃ o train.py (ngáº¯n gá»n)

train_py = "/content/OpenPAR/PromptPAR/train.py"

with open(train_py, "r") as f:
    code = f.read()

if "torch.load" not in code:
    print("Äang thÃªm logic resume...")

    code = code.replace(
        "model = TransformerClassifier(clip_model,train_set.attr_num,train_set.attributes)",
        """model = TransformerClassifier(clip_model,train_set.attr_num,train_set.attributes)

    # ---- AUTO RESUME ----
    start_epoch = 1
    if args.checkpoint:
        import glob
        ckpts = glob.glob(os.path.join(args.dir, "epoch*.pth"))
        if ckpts:
            latest = max(ckpts, key=os.path.getctime)
            print("Resume tá»«:", latest)
            ckpt = torch.load(latest, map_location=device)
            model.load_state_dict(ckpt["model_state_dict"])
            clip_model.load_state_dict(ckpt["clip_model"])
            try:
                optimizer.load_state_dict(ckpt["optimizer"])
            except:
                pass
            start_epoch = ckpt["epoch"] + 1
    """
    )

    code = code.replace(
        "for i in range(1, epoch+1):",
        "for i in range(start_epoch, epoch+1):"
    )

    with open(train_py, "w") as f:
        f.write(code)

    print("Patch xong")
else:
    print("File Ä‘Ã£ cÃ³ logic resume, bá» qua")

# Commented out IPython magic to ensure Python compatibility.
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 10: TRAINING (LÆ¯U THáº²NG VÃ€O DRIVE)                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# %cd /content/OpenPAR/PromptPAR

import os, glob

# Táº¡o thÆ° má»¥c trong Drive
CHECKPOINT_DIR = "/content/drive/MyDrive/PromptPAR_checkpoints/Vit_Market1501"
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

# Táº¡o symlink tá»« local output sang Drive
LOCAL_OUTPUT = "/content/OpenPAR/PromptPAR/exp_result"
if os.path.exists(LOCAL_OUTPUT):
    if os.path.islink(LOCAL_OUTPUT):
        os.unlink(LOCAL_OUTPUT) # Remove symbolic link directly
    elif os.path.isdir(LOCAL_OUTPUT):
        import shutil
        shutil.rmtree(LOCAL_OUTPUT) # Remove directory
os.symlink(CHECKPOINT_DIR, LOCAL_OUTPUT)

# Tá»± Ä‘á»™ng tÃ¬m checkpoint má»›i nháº¥t Ä‘á»ƒ resume
checkpoint_files = glob.glob(f"{CHECKPOINT_DIR}/*.pth")
resume_arg = ""
if checkpoint_files:
    latest_ckpt = max(checkpoint_files, key=os.path.getctime)
    print(f"TÃ¬m tháº¥y checkpoint má»›i nháº¥t â†’ tiáº¿p tá»¥c tá»«: {os.path.basename(latest_ckpt)}")
    resume_arg = f"--checkpoint"
else:
    print("KhÃ´ng cÃ³ checkpoint â†’ train tá»« Ä‘áº§u")

print(f" Symlink: {LOCAL_OUTPUT} â†’ {CHECKPOINT_DIR}")
print(" Báº¯t Ä‘áº§u training.. .\n")

cmd = f"""
python train.py Market1501 \
--batchsize 32 \
--epoch 60 \
--height 224 \
--width 224 \
--lr 5e-5 \
--weight_decay 1e-4 \
--clip_lr 1e-5 \
--clip_weight_decay 1e-4 \
--text_prompt 3 \
--vis_prompt 20 \
--vis_depth 12 \
--div_num 4 \
--overlap_row 2 \
--mm_layers 1 \
--smooth_param 0.1 \
--ag_threshold 0.5 \
--train_split trainval \
--valid_split test \
--save_freq 5 \
--use_div \
--use_textprompt \
--use_mm_former \
--use_GL \
--dir {CHECKPOINT_DIR} \
{resume_arg}
"""

print(cmd)
!{cmd}