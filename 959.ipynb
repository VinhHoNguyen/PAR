{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWbA2O4infGj",
        "outputId": "ba9125fa-0aea-4a1f-867e-753430c4b1f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " Checkpoint dir: /content/drive/MyDrive/PromptPAR_checkpoints/Vit_Market1501\n"
          ]
        }
      ],
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘  CELL 1: MOUNT GOOGLE DRIVE                                  â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/PromptPAR_checkpoints/Vit_Market1501'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs('/data/jinjiandong', exist_ok=True)\n",
        "\n",
        "print(f\" Checkpoint dir: {CHECKPOINT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q4ArAIGn2cw",
        "outputId": "c003d94b-b0af-4c09-9258-d0bbbbb90d52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'OpenPAR'...\n",
            "remote: Enumerating objects: 2108, done.\u001b[K\n",
            "remote: Counting objects: 100% (380/380), done.\u001b[K\n",
            "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
            "remote: Total 2108 (delta 331), reused 248 (delta 248), pack-reused 1728 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2108/2108), 80.72 MiB | 18.76 MiB/s, done.\n",
            "Resolving deltas: 100% (1208/1208), done.\n",
            " Cloned OpenPAR\n",
            "Cloning into 'Market-1501_Attribute'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 109 (delta 0), reused 0 (delta 0), pack-reused 106 (from 1)\u001b[K\n",
            "Receiving objects: 100% (109/109), 138.96 KiB | 5.34 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n",
            " Cloned Market-1501_Attribute\n",
            "total 32\n",
            "drwxr-xr-x  1 root root 4096 Dec 10 02:50 .\n",
            "drwxr-xr-x  1 root root 4096 Dec 10 02:26 ..\n",
            "drwxr-xr-x  4 root root 4096 Nov 20 14:30 .config\n",
            "drwxr-xr-x  4 root root 4096 Dec 10 02:49 data\n",
            "drwx------  5 root root 4096 Dec 10 02:26 drive\n",
            "drwxr-xr-x  3 root root 4096 Dec 10 02:50 Market-1501_Attribute\n",
            "drwxr-xr-x 12 root root 4096 Dec 10 02:50 OpenPAR\n",
            "drwxr-xr-x  1 root root 4096 Nov 20 14:30 sample_data\n"
          ]
        }
      ],
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘  CELL 3: CLONE REPOSITORIES                                  â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "%cd /content\n",
        "\n",
        "# Clone OpenPAR\n",
        "if os.path.exists('/content/OpenPAR'):\n",
        "    shutil.rmtree('/content/OpenPAR')\n",
        "! git clone https://github.com/Event-AHU/OpenPAR.git\n",
        "print(\" Cloned OpenPAR\")\n",
        "\n",
        "# Clone Market-1501 Attributes\n",
        "if os.path.exists('/content/Market-1501_Attribute'):\n",
        "    shutil.rmtree('/content/Market-1501_Attribute')\n",
        "!git clone https://github.com/vana77/Market-1501_Attribute.git\n",
        "print(\" Cloned Market-1501_Attribute\")\n",
        "\n",
        "# Verify\n",
        "!ls -la /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c3YQRE4n6j8",
        "outputId": "136af788-b6da-4f23-aca6-df93b63b7966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/OpenPAR/PromptPAR\n"
          ]
        }
      ],
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘  CELL 4: CÃ€I Äáº¶T DEPENDENCIES                                â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "%cd /content/OpenPAR/PromptPAR\n",
        "\n",
        "! pip install -q ftfy regex tqdm easydict scipy\n",
        "!pip install -q git+https://github.com/openai/CLIP.git\n",
        "\n",
        "print(\"\\n Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgDn9wL7oE29"
      },
      "outputs": [],
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘  CELL 5: Táº¢I MARKET-1501 DATASET                             â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import os\n",
        "\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "%cd /content/data\n",
        "\n",
        "# Táº£i Market-1501\n",
        "if not os.path. exists('/content/data/Market-1501-v15.09.15'):\n",
        "    print(\" Äang táº£i Market-1501...\")\n",
        "    !pip install -q gdown\n",
        "    !gdown \"https://drive.google.com/uc?id=0B8-rUzbwVRk0c054eEozWG9COHM\" -O Market-1501-v15.09.15.zip --fuzzy\n",
        "    !unzip -q Market-1501-v15.09.15.zip\n",
        "    !rm Market-1501-v15.09.15. zip\n",
        "    print(\" ÄÃ£ táº£i vÃ  giáº£i nÃ©n Market-1501\")\n",
        "else:\n",
        "    print(\" Market-1501 Ä‘Ã£ tá»“n táº¡i\")\n",
        "\n",
        "! ls -la /content/data/Market-1501-v15.09.15/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uis6wLZeoIcG"
      },
      "outputs": [],
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘  CELL 6: Táº¢I VIT PRETRAINED MODEL                            â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import os\n",
        "\n",
        "os.makedirs('/data/jinjiandong', exist_ok=True)\n",
        "\n",
        "vit_path = '/data/jinjiandong/jx_vit_base_p16_224-80ecf9dd.pth'\n",
        "\n",
        "if not os.path.exists(vit_path):\n",
        "    print(\"ğŸ”„ Äang táº£i ViT pretrained...\")\n",
        "    !wget -q https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth -O {vit_path}\n",
        "\n",
        "if os.path.exists(vit_path):\n",
        "    size = os.path.getsize(vit_path) / 1024 / 1024\n",
        "    print(f\" ViT pretrained: {size:.1f} MB\")\n",
        "else:\n",
        "    print(\" Táº£i ViT tháº¥t báº¡i!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8y5J6db6oM6H",
        "outputId": "4852f157-7a0a-4465-ad68-2ffe6333457a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloned fresh OpenPAR\n",
            " ÄÃ£ sá»­a clip/model.py\n",
            " ÄÃ£ sá»­a dataset/AttrDataset.py\n",
            "\n",
            " Verify model.py:\n",
            "   Line 17: datasets_attrnum={'PA100k':26,'RAPV1':51,'PETA':35,'PETAzs':35,'UPAR':...\n",
            "    Market1501 = 27 attributes\n"
          ]
        }
      ],
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘  CELL 7: Sá»¬A CODE PROMPTPAR Äá»‚ Há»– TRá»¢ MARKET1501             â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Clone láº¡i Ä‘á»ƒ Ä‘áº£m báº£o code sáº¡ch\n",
        "%cd /content\n",
        "if os.path.exists('/content/OpenPAR'):\n",
        "    shutil.rmtree('/content/OpenPAR')\n",
        "! git clone -q https://github.com/Event-AHU/OpenPAR.git\n",
        "print(\"Cloned fresh OpenPAR\")\n",
        "\n",
        "# ========== 1. Sá»¬A clip/model.py ==========\n",
        "model_py = '/content/OpenPAR/PromptPAR/clip/model.py'\n",
        "\n",
        "with open(model_py, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "new_lines = []\n",
        "for line in lines:\n",
        "    # Sá»­a dÃ²ng assert\n",
        "    if \"assert args.dataset in [\" in line and \"Market1501\" not in line:\n",
        "        new_lines.append(\"assert args.dataset in ['PA100k', 'RAPV1','RAPV2','PETA','WIDER','PETAzs','RAPzs','UPAR','YCJC','RAPV1Expand','Market1501'], \\\\\\n\")\n",
        "    # Sá»­a dÃ²ng datasets_attrnum\n",
        "    elif \"datasets_attrnum=\" in line and \"Market1501\" not in line:\n",
        "        new_lines. append(\"datasets_attrnum={'PA100k':26,'RAPV1':51,'PETA':35,'PETAzs':35,'UPAR':40,'RAPzs':53,'RAPV2':54,'WIDER':14,'RAPV1Expand':51,'Market1501':27}\\n\")\n",
        "    else:\n",
        "        new_lines.append(line)\n",
        "\n",
        "with open(model_py, 'w') as f:\n",
        "    f. writelines(new_lines)\n",
        "\n",
        "print(\" ÄÃ£ sá»­a clip/model.py\")\n",
        "\n",
        "# ========== 2. Sá»¬A dataset/AttrDataset.py ==========\n",
        "attr_py = '/content/OpenPAR/PromptPAR/dataset/AttrDataset.py'\n",
        "\n",
        "with open(attr_py, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# ThÃªm Market1501\n",
        "content = content.replace(\n",
        "    \"['PA100k', 'RAPV1','RAPV2','PETA','WIDER','RAPzs','PETAzs','UPAR','YCJC',]\",\n",
        "    \"['PA100k', 'RAPV1','RAPV2','PETA','WIDER','RAPzs','PETAzs','UPAR','YCJC','Market1501']\"\n",
        ")\n",
        "\n",
        "# Sá»­a Ä‘Æ°á»ng dáº«n dataset\n",
        "content = content. replace(\n",
        "    \"dataset_dir='/data/jinjiandong/datasets/'\",\n",
        "    \"dataset_dir='/content/data/'\"\n",
        ")\n",
        "\n",
        "with open(attr_py, 'w') as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\" ÄÃ£ sá»­a dataset/AttrDataset.py\")\n",
        "\n",
        "# ========== VERIFY ==========\n",
        "print(\"\\n Verify model.py:\")\n",
        "with open(model_py, 'r') as f:\n",
        "    for i, line in enumerate(f, 1):\n",
        "        if 'datasets_attrnum' in line:\n",
        "            print(f\"   Line {i}: {line.strip()[:70]}...\")\n",
        "            if \"'Market1501':27\" in line:\n",
        "                print(\"    Market1501 = 27 attributes\")\n",
        "            else:\n",
        "                print(\"    ChÆ°a Ä‘Ãºng!\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NtRBMqo7rgVT",
        "outputId": "29d77d70-3afc-4f98-b4d1-f4eed0ae260d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " TRÆ¯á»šC KHI Sá»¬A:\n",
            "   args = parser.parse_args()\n",
            "   assert args.dataset in ['PA100k', 'RAPV1','RAPV2','PETA','WIDER','PETAzs','RAPzs\n",
            "       f'dataset name {args.dataset} is not exist,The legal name is PA100k,RAPV1,RA\n",
            "   datasets_attrnum={'PA100k':26,'RAPV1':51,'PETA':35,'PETAzs':35,'UPAR':40,'RAPzs'\n",
            "   attr_num=datasets_attrnum[args.dataset]\n",
            "   class Bottleneck(nn.Module):\n",
            "\n",
            " SAU KHI Sá»¬A:\n",
            "   args = parser.parse_args()\n",
            "   assert args.dataset in ['PA100k', 'RAPV1','RAPV2','PETA','WIDER','PETAzs','RAPzs\n",
            "       f'dataset name {args.dataset} is not exist,The legal name is PA100k,RAPV1,RA\n",
            "   datasets_attrnum={'PA100k':26,'RAPV1':51,'PETA':35,'PETAzs':35,'UPAR':40,'RAPzs'\n",
            "   attr_num=datasets_attrnum[args.dataset]\n",
            "   class Bottleneck(nn.Module):\n",
            "\n",
            " OK!  Market1501 = 27\n"
          ]
        }
      ],
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘  CELL 7 FIX: Sá»¬A TRá»°C TIáº¾P FILE MODEL.PY                     â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "model_py = '/content/OpenPAR/PromptPAR/clip/model.py'\n",
        "\n",
        "with open(model_py, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Hiá»ƒn thá»‹ trÆ°á»›c khi sá»­a\n",
        "print(\" TRÆ¯á»šC KHI Sá»¬A:\")\n",
        "for line in content.split('\\n')[13:19]:\n",
        "    print(f\"   {line[:80]}\")\n",
        "\n",
        "# Sá»­a táº¥t cáº£ cÃ¡c pattern cÃ³ thá»ƒ\n",
        "replacements = [\n",
        "    # Pattern 1\n",
        "    (\"datasets_attrnum={'PA100k':26,'RAPV1':51,'PETA':35,'PETAzs':35,'UPAR':40,'RAPzs':53,'RAPV2':54,'WIDER':14,\\\"RAPV1Expand\\\":51}\",\n",
        "     \"datasets_attrnum={'PA100k':26,'RAPV1':51,'PETA':35,'PETAzs':35,'UPAR':40,'RAPzs':53,'RAPV2':54,'WIDER':14,'RAPV1Expand':51,'Market1501':27}\"),\n",
        "    # Pattern 2 (náº¿u Ä‘Ã£ cÃ³ Market1501 nhÆ°ng sai sá»‘)\n",
        "    (\"'Market1501':39\", \"'Market1501':27\"),\n",
        "    # Assert pattern\n",
        "    (\"['PA100k', 'RAPV1','RAPV2','PETA','WIDER','PETAzs','RAPzs','UPAR','YCJC',\\\"RAPV1Expand\\\"]\",\n",
        "     \"['PA100k', 'RAPV1','RAPV2','PETA','WIDER','PETAzs','RAPzs','UPAR','YCJC','RAPV1Expand','Market1501']\"),\n",
        "]\n",
        "\n",
        "for old, new in replacements:\n",
        "    if old in content:\n",
        "        content = content. replace(old, new)\n",
        "        print(f\"\\n ÄÃ£ thay tháº¿ pattern\")\n",
        "\n",
        "with open(model_py, 'w') as f:\n",
        "    f.write(content)\n",
        "\n",
        "# Hiá»ƒn thá»‹ sau khi sá»­a\n",
        "print(\"\\n SAU KHI Sá»¬A:\")\n",
        "with open(model_py, 'r') as f:\n",
        "    content = f.read()\n",
        "for line in content.split('\\n')[13:19]:\n",
        "    print(f\"   {line[:80]}\")\n",
        "\n",
        "# Verify\n",
        "if \"'Market1501':27\" in content:\n",
        "    print(\"\\n OK!  Market1501 = 27\")\n",
        "else:\n",
        "    print(\"\\n ChÆ°a sá»­a Ä‘Æ°á»£c!  Cáº§n sá»­a thá»§ cÃ´ng\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DmBJ1EHXoSAC",
        "outputId": "fdb7c5e6-a057-4b60-d5a1-af9e5b79c832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kiá»ƒm tra files...\n",
            "Files OK\n",
            "ğŸ“Š Attributes tá»« . mat: 27\n",
            "Train images: 12936\n",
            "Test images: 13115\n",
            "\n",
            " Processing training images...\n",
            " Train samples: 7421\n",
            " Processing test images...\n",
            " Test samples: 6868\n",
            "\n",
            " Saved: /content/data/Market1501/pad. pkl\n",
            "Created symlinks\n",
            "\n",
            "============================================================\n",
            " DATASET SUMMARY\n",
            "============================================================\n",
            "Labels shape: (14289, 27)\n",
            "Attributes: 27\n",
            "Train: 7421\n",
            "Test: 6868\n",
            "pad.pkl: 2.25 MB\n",
            "\n",
            " OK!  attributes = labels = 27\n"
          ]
        }
      ],
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘  CELL 8: Táº O DATASET PICKLE CHO MARKET1501                   â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import shutil\n",
        "from easydict import EasyDict\n",
        "\n",
        "# ========== PATHS ==========\n",
        "MARKET_DIR = '/content/data/Market-1501-v15.09.15'\n",
        "ATTR_FILE = '/content/Market-1501_Attribute/market_attribute.mat'\n",
        "SAVE_DIR = '/content/data/Market1501'\n",
        "\n",
        "# ========== CHECK FILES ==========\n",
        "print(\"Kiá»ƒm tra files...\")\n",
        "if not os.path. exists(MARKET_DIR):\n",
        "    raise Exception(\"ChÆ°a cÃ³ Market-1501!  Cháº¡y Cell 5 trÆ°á»›c.\")\n",
        "if not os.path. exists(ATTR_FILE):\n",
        "    raise Exception(\"ChÆ°a cÃ³ market_attribute.mat! Cháº¡y Cell 3 trÆ°á»›c.\")\n",
        "print(\"Files OK\")\n",
        "\n",
        "# ========== LOAD . MAT FILE ==========\n",
        "mat_data = scipy. io.loadmat(ATTR_FILE)\n",
        "market_attr = mat_data['market_attribute'][0][0]\n",
        "train_attr_mat = market_attr['train'][0][0]\n",
        "test_attr_mat = market_attr['test'][0][0]\n",
        "\n",
        "# Láº¥y tÃªn attributes Tá»ª FILE . MAT (27 attributes)\n",
        "attr_names = [name for name in train_attr_mat.dtype. names if name != 'image_index']\n",
        "print(f\"ğŸ“Š Attributes tá»« . mat: {len(attr_names)}\")\n",
        "\n",
        "# ========== GET IMAGE LISTS ==========\n",
        "train_dir = os.path. join(MARKET_DIR, 'bounding_box_train')\n",
        "test_dir = os.path. join(MARKET_DIR, 'bounding_box_test')\n",
        "\n",
        "train_images = sorted([f for f in os. listdir(train_dir)\n",
        "                      if f.endswith('.jpg') and not f.startswith('-1')])\n",
        "test_images = sorted([f for f in os. listdir(test_dir)\n",
        "                     if f.endswith('.jpg') and not f.startswith('-1') and not f.startswith('0000')])\n",
        "\n",
        "print(f\"Train images: {len(train_images)}\")\n",
        "print(f\"Test images: {len(test_images)}\")\n",
        "\n",
        "# ========== CREATE DATASET ==========\n",
        "image_name_list = []\n",
        "label_list = []\n",
        "train_idx = []\n",
        "test_idx = []\n",
        "idx = 0\n",
        "\n",
        "# Process TRAINING images\n",
        "print(\"\\n Processing training images...\")\n",
        "for img_name in train_images:\n",
        "    try:\n",
        "        identity_id = int(img_name.split('_')[0])\n",
        "        if 1 <= identity_id <= 751:\n",
        "            image_name_list.append(os.path.join('bounding_box_train', img_name))\n",
        "            attr_idx = identity_id - 1\n",
        "            label = [int(train_attr_mat[attr_name][0][attr_idx]) - 1\n",
        "                     if int(train_attr_mat[attr_name][0][attr_idx]) > 0 else 0\n",
        "                     for attr_name in attr_names]\n",
        "            label_list.append(label)\n",
        "            train_idx.append(idx)\n",
        "            idx += 1\n",
        "    except:\n",
        "        continue\n",
        "print(f\" Train samples: {len(train_idx)}\")\n",
        "\n",
        "# Process TEST images\n",
        "print(\" Processing test images...\")\n",
        "for img_name in test_images:\n",
        "    try:\n",
        "        identity_id = int(img_name.split('_')[0])\n",
        "        if 1 <= identity_id <= 750:\n",
        "            image_name_list.append(os.path. join('bounding_box_test', img_name))\n",
        "            attr_idx = identity_id - 1\n",
        "            label = [int(test_attr_mat[attr_name][0][attr_idx]) - 1\n",
        "                     if int(test_attr_mat[attr_name][0][attr_idx]) > 0 else 0\n",
        "                     for attr_name in attr_names]\n",
        "            label_list. append(label)\n",
        "            test_idx. append(idx)\n",
        "            idx += 1\n",
        "    except:\n",
        "        continue\n",
        "print(f\" Test samples: {len(test_idx)}\")\n",
        "\n",
        "# ========== CREATE EASYDICT ==========\n",
        "dataset = EasyDict()\n",
        "dataset.description = 'market1501'\n",
        "dataset.root = SAVE_DIR\n",
        "dataset. image_name = image_name_list\n",
        "dataset.label = np.array(label_list, dtype=np.float32)\n",
        "dataset. attributes = attr_names  # 27 attributes tá»« .mat\n",
        "\n",
        "dataset.partition = EasyDict()\n",
        "dataset.partition. train = np.array(train_idx, dtype=np.int64)\n",
        "dataset.partition.val = np.array([], dtype=np.int64)\n",
        "dataset. partition.test = np.array(test_idx, dtype=np.int64)\n",
        "dataset.partition.trainval = np.array(train_idx, dtype=np.int64)\n",
        "\n",
        "# Weights\n",
        "train_labels = dataset.label[dataset. partition.train]\n",
        "label_ratio = np.clip(np.mean(train_labels, axis=0), 0.01, 0.99)\n",
        "dataset.weight_train = np.exp(-label_ratio). astype(np. float32)\n",
        "dataset.weight_trainval = dataset.weight_train. copy()\n",
        "\n",
        "# ========== SAVE ==========\n",
        "if os. path.exists(SAVE_DIR):\n",
        "    shutil.rmtree(SAVE_DIR)\n",
        "os.makedirs(SAVE_DIR)\n",
        "\n",
        "pkl_path = os.path. join(SAVE_DIR, 'pad. pkl')\n",
        "with open(pkl_path, 'wb') as f:\n",
        "    pickle.dump(dataset, f)\n",
        "print(f\"\\n Saved: {pkl_path}\")\n",
        "\n",
        "os.symlink(train_dir, os. path.join(SAVE_DIR, 'bounding_box_train'))\n",
        "os.symlink(test_dir, os. path.join(SAVE_DIR, 'bounding_box_test'))\n",
        "print(\"Created symlinks\")\n",
        "\n",
        "# ========== VERIFY ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\" DATASET SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Labels shape: {dataset. label.shape}\")\n",
        "print(f\"Attributes: {len(dataset. attributes)}\")\n",
        "print(f\"Train: {len(dataset.partition. train)}\")\n",
        "print(f\"Test: {len(dataset.partition. test)}\")\n",
        "\n",
        "# Verify file exists\n",
        "if os.path.exists(pkl_path):\n",
        "    size = os.path. getsize(pkl_path) / 1024 / 1024\n",
        "    print(f\"pad.pkl: {size:.2f} MB\")\n",
        "\n",
        "if len(dataset.attributes) == dataset.label.shape[1]:\n",
        "    print(f\"\\n OK!  attributes = labels = {len(dataset.attributes)}\")\n",
        "else:\n",
        "    print(f\"\\n MISMATCH!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8hPGN6KfxPX8",
        "outputId": "2bf6de09-57a9-4b34-c3fd-9aacfc9a601c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attributes: 27\n",
            "Train: 12936, Test: 13115\n",
            " Train: 7421, Test: 6868\n",
            "\n",
            "============================================================\n",
            "CREATED: /content/data/Market1501/pad.pkl (2.25 MB)\n",
            "Labels: (14289, 27)\n",
            "Attributes: 27\n",
            "\n",
            " THÃ€NH CÃ”NG! Cháº¡y Cell 10 Ä‘á»ƒ training\n"
          ]
        }
      ],
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘  Táº O PAD.PKL                                                 â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import shutil\n",
        "from easydict import EasyDict\n",
        "\n",
        "# ========== PATHS ==========\n",
        "MARKET_DIR = '/content/data/Market-1501-v15.09.15'\n",
        "ATTR_FILE = '/content/Market-1501_Attribute/market_attribute.mat'\n",
        "SAVE_DIR = '/content/data/Market1501'\n",
        "\n",
        "# XÃ³a vÃ  táº¡o láº¡i thÆ° má»¥c\n",
        "if os.path.exists(SAVE_DIR):\n",
        "    shutil.rmtree(SAVE_DIR)\n",
        "os.makedirs(SAVE_DIR)\n",
        "\n",
        "# ========== LOAD . MAT FILE ==========\n",
        "mat_data = scipy.io. loadmat(ATTR_FILE)\n",
        "market_attr = mat_data['market_attribute'][0][0]\n",
        "train_attr_mat = market_attr['train'][0][0]\n",
        "test_attr_mat = market_attr['test'][0][0]\n",
        "\n",
        "# Láº¥y 27 attributes tá»« . mat\n",
        "attr_names = [name for name in train_attr_mat.dtype.names if name != 'image_index']\n",
        "print(f\"Attributes: {len(attr_names)}\")\n",
        "\n",
        "# ========== GET IMAGE LISTS ==========\n",
        "train_dir = os.path. join(MARKET_DIR, 'bounding_box_train')\n",
        "test_dir = os.path. join(MARKET_DIR, 'bounding_box_test')\n",
        "\n",
        "train_images = sorted([f for f in os.listdir(train_dir)\n",
        "                      if f.endswith('.jpg') and not f.startswith('-1')])\n",
        "test_images = sorted([f for f in os. listdir(test_dir)\n",
        "                     if f.endswith('.jpg') and not f.startswith('-1') and not f.startswith('0000')])\n",
        "\n",
        "print(f\"Train: {len(train_images)}, Test: {len(test_images)}\")\n",
        "\n",
        "# ========== CREATE DATASET ==========\n",
        "image_name_list = []\n",
        "label_list = []\n",
        "train_idx = []\n",
        "test_idx = []\n",
        "idx = 0\n",
        "\n",
        "# TRAINING\n",
        "for img_name in train_images:\n",
        "    try:\n",
        "        identity_id = int(img_name.split('_')[0])\n",
        "        if 1 <= identity_id <= 751:\n",
        "            image_name_list.append(os.path.join('bounding_box_train', img_name))\n",
        "            attr_idx = identity_id - 1\n",
        "            label = [max(0, int(train_attr_mat[attr_name][0][attr_idx]) - 1) for attr_name in attr_names]\n",
        "            label_list.append(label)\n",
        "            train_idx.append(idx)\n",
        "            idx += 1\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# TEST\n",
        "for img_name in test_images:\n",
        "    try:\n",
        "        identity_id = int(img_name.split('_')[0])\n",
        "        if 1 <= identity_id <= 750:\n",
        "            image_name_list.append(os.path. join('bounding_box_test', img_name))\n",
        "            attr_idx = identity_id - 1\n",
        "            label = [max(0, int(test_attr_mat[attr_name][0][attr_idx]) - 1) for attr_name in attr_names]\n",
        "            label_list.append(label)\n",
        "            test_idx.append(idx)\n",
        "            idx += 1\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(f\" Train: {len(train_idx)}, Test: {len(test_idx)}\")\n",
        "\n",
        "# ========== CREATE EASYDICT ==========\n",
        "dataset = EasyDict()\n",
        "dataset.description = 'market1501'\n",
        "dataset.root = SAVE_DIR\n",
        "dataset. image_name = image_name_list\n",
        "dataset.label = np.array(label_list, dtype=np.float32)\n",
        "dataset. attributes = attr_names\n",
        "\n",
        "dataset.partition = EasyDict()\n",
        "dataset.partition.train = np.array(train_idx, dtype=np.int64)\n",
        "dataset.partition.val = np.array([], dtype=np.int64)\n",
        "dataset. partition.test = np.array(test_idx, dtype=np.int64)\n",
        "dataset.partition.trainval = np.array(train_idx, dtype=np.int64)\n",
        "\n",
        "# Weights\n",
        "train_labels = dataset.label[dataset.partition.train]\n",
        "label_ratio = np.clip(np.mean(train_labels, axis=0), 0.01, 0.99)\n",
        "dataset.weight_train = np.exp(-label_ratio). astype(np.float32)\n",
        "dataset.weight_trainval = dataset.weight_train. copy()\n",
        "\n",
        "# ========== SAVE PICKLE ==========\n",
        "pkl_path = os. path.join(SAVE_DIR, 'pad.pkl')\n",
        "with open(pkl_path, 'wb') as f:\n",
        "    pickle.dump(dataset, f)\n",
        "\n",
        "# ========== CREATE SYMLINKS ==========\n",
        "os.symlink(train_dir, os. path.join(SAVE_DIR, 'bounding_box_train'))\n",
        "os.symlink(test_dir, os. path.join(SAVE_DIR, 'bounding_box_test'))\n",
        "\n",
        "# ========== VERIFY ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "if os.path.exists(pkl_path):\n",
        "    size = os.path. getsize(pkl_path) / 1024 / 1024\n",
        "    print(f\"CREATED: {pkl_path} ({size:.2f} MB)\")\n",
        "    print(f\"Labels: {dataset.label.shape}\")\n",
        "    print(f\"Attributes: {len(dataset.attributes)}\")\n",
        "    print(f\"\\n THÃ€NH CÃ”NG! Cháº¡y Cell 10 Ä‘á»ƒ training\")\n",
        "else:\n",
        "    print(\"THáº¤T Báº I!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-56x_dyR4S8o",
        "outputId": "5661cb66-aadd-47b9-db3e-63f2fc41ad36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train.py Ä‘Ã£ Ä‘Æ°á»£c chá»‰nh Ä‘á»ƒ lÆ°u vÃ o Drive\n"
          ]
        }
      ],
      "source": [
        "# --- PATCH: lÆ°u checkpoint tháº³ng vÃ o Drive, khÃ´ng dÃ¹ng timestamp ---\n",
        "\n",
        "train_py = \"/content/OpenPAR/PromptPAR/train.py\"\n",
        "\n",
        "with open(train_py, \"r\") as f:\n",
        "    code = f.read()\n",
        "\n",
        "# Block cÅ© dÃ¹ng timestamp\n",
        "old = \"\"\"start_time=time_str()\n",
        "    print(f'start_time is {start_time}')\n",
        "    log_dir = os.path.join('logs', args.dataset)\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "    log_dir = os.path.join(log_dir, start_time)\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\"\"\"\n",
        "\n",
        "# Block má»›i: láº¥y Ä‘Æ°á»ng dáº«n tá»« --dir\n",
        "new = \"\"\"log_dir = args.dir\n",
        "    os.makedirs(log_dir, exist_ok=True)\"\"\"\n",
        "\n",
        "# Thay tháº¿\n",
        "code = code.replace(old, new)\n",
        "\n",
        "with open(train_py, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"train.py Ä‘Ã£ Ä‘Æ°á»£c chá»‰nh Ä‘á»ƒ lÆ°u vÃ o Drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Gi5qE0EGDnaq",
        "outputId": "b370cc46-1f18-4400-99e6-df64aebcb460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Äang thÃªm logic resume...\n",
            "Patch xong\n"
          ]
        }
      ],
      "source": [
        "# PATCH: thÃªm resume logic vÃ o train.py (ngáº¯n gá»n)\n",
        "\n",
        "train_py = \"/content/OpenPAR/PromptPAR/train.py\"\n",
        "\n",
        "with open(train_py, \"r\") as f:\n",
        "    code = f.read()\n",
        "\n",
        "if \"torch.load\" not in code:\n",
        "    print(\"Äang thÃªm logic resume...\")\n",
        "\n",
        "    code = code.replace(\n",
        "        \"model = TransformerClassifier(clip_model,train_set.attr_num,train_set.attributes)\",\n",
        "        \"\"\"model = TransformerClassifier(clip_model,train_set.attr_num,train_set.attributes)\n",
        "\n",
        "    # ---- AUTO RESUME ----\n",
        "    start_epoch = 1\n",
        "    if args.checkpoint:\n",
        "        import glob\n",
        "        ckpts = glob.glob(os.path.join(args.dir, \"epoch*.pth\"))\n",
        "        if ckpts:\n",
        "            latest = max(ckpts, key=os.path.getctime)\n",
        "            print(\"Resume tá»«:\", latest)\n",
        "            ckpt = torch.load(latest, map_location=device)\n",
        "            model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "            clip_model.load_state_dict(ckpt[\"clip_model\"])\n",
        "            try:\n",
        "                optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "            except:\n",
        "                pass\n",
        "            start_epoch = ckpt[\"epoch\"] + 1\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    code = code.replace(\n",
        "        \"for i in range(1, epoch+1):\",\n",
        "        \"for i in range(start_epoch, epoch+1):\"\n",
        "    )\n",
        "\n",
        "    with open(train_py, \"w\") as f:\n",
        "        f.write(code)\n",
        "\n",
        "    print(\"Patch xong\")\n",
        "else:\n",
        "    print(\"File Ä‘Ã£ cÃ³ logic resume, bá» qua\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0osnB0apIcWh",
        "outputId": "aba592fa-f29b-4f88-b2a6-798b55f73dfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Patched: allow EasyDict in torch.load\n"
          ]
        }
      ],
      "source": [
        "# === FIX PyTorch 2.6+ checkpoint loading on Colab ===\n",
        "from easydict import EasyDict\n",
        "import torch.serialization\n",
        "\n",
        "torch.serialization.add_safe_globals([EasyDict])\n",
        "print(\"âœ… Patched: allow EasyDict in torch.load\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K6T7uKsfoYiI",
        "outputId": "cc49ce1d-d2d8-40e2-933f-6393bf5086e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/OpenPAR/PromptPAR\n",
            "TÃ¬m tháº¥y checkpoint má»›i nháº¥t â†’ tiáº¿p tá»¥c tá»«: epoch5.pth\n",
            " Symlink: /content/OpenPAR/PromptPAR/exp_result â†’ /content/drive/MyDrive/PromptPAR_checkpoints/Vit_Market1501\n",
            " Báº¯t Ä‘áº§u training.. .\n",
            "\n",
            "\n",
            "python train.py Market1501 --batchsize 32 --epoch 60 --height 224 --width 224 --lr 5e-5 --weight_decay 1e-4 --clip_lr 1e-5 --clip_weight_decay 1e-4 --text_prompt 3 --vis_prompt 20 --vis_depth 12 --div_num 4 --overlap_row 2 --mm_layers 1 --smooth_param 0.1 --ag_threshold 0.5 --train_split trainval --valid_split test --save_freq 5 --use_div --use_textprompt --use_mm_former --use_GL --dir /content/drive/MyDrive/PromptPAR_checkpoints/Vit_Market1501 --checkpoint\n",
            "\n",
            "redirector stdout\n",
            "OrderedDict([('dataset', 'Market1501'),\n",
            "             ('batchsize', 32),\n",
            "             ('epoch', 60),\n",
            "             ('height', 224),\n",
            "             ('width', 224),\n",
            "             ('lr', 5e-05),\n",
            "             ('weight_decay', 0.0001),\n",
            "             ('ag_threshold', 0.5),\n",
            "             ('smooth_param', 0.1),\n",
            "             ('use_div', True),\n",
            "             ('use_vismask', False),\n",
            "             ('use_GL', True),\n",
            "             ('use_textprompt', True),\n",
            "             ('use_mm_former', True),\n",
            "             ('mm_layers', 1),\n",
            "             ('div_num', 4),\n",
            "             ('overlap_row', 2),\n",
            "             ('text_prompt', 3),\n",
            "             ('vis_prompt', 20),\n",
            "             ('vis_depth', 12),\n",
            "             ('clip_lr', 1e-05),\n",
            "             ('clip_weight_decay', 0.0001),\n",
            "             ('mmformer_update_parameters',\n",
            "              ['word_embed', 'visual_embed', 'weight_layer', 'bn', 'norm']),\n",
            "             ('clip_update_parameters',\n",
            "              ['prompt_deep',\n",
            "               'prompt_text_deep',\n",
            "               'part_class_embedding',\n",
            "               'agg_bn',\n",
            "               'softmax_model']),\n",
            "             ('train_split', 'trainval'),\n",
            "             ('valid_split', 'test'),\n",
            "             ('redirector', True),\n",
            "             ('save_freq', 5),\n",
            "             ('checkpoint', True),\n",
            "             ('dir',\n",
            "              '/content/drive/MyDrive/PromptPAR_checkpoints/Vit_Market1501')])\n",
            "------------------------------------------------------------\n",
            "train set: Market1501 trainval, test set: test\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "trainval set: 7421, test set: 6868, attr_num : 27\n",
            "Resume tá»«: /content/drive/MyDrive/PromptPAR_checkpoints/Vit_Market1501/epoch5.pth\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/OpenPAR/PromptPAR/train.py\", line 187, in <module>\n",
            "    main(args)\n",
            "  File \"/content/OpenPAR/PromptPAR/train.py\", line 71, in main\n",
            "    ckpt = torch.load(latest, map_location=device)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1529, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL easydict.EasyDict was not an allowed global by default. Please use `torch.serialization.add_safe_globals([easydict.EasyDict])` or the `torch.serialization.safe_globals([easydict.EasyDict])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
          ]
        }
      ],
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘  CELL 10: TRAINING (LÆ¯U THáº²NG VÃ€O DRIVE)                     â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "%cd /content/OpenPAR/PromptPAR\n",
        "\n",
        "import os, glob\n",
        "\n",
        "# Táº¡o thÆ° má»¥c trong Drive\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/PromptPAR_checkpoints/Vit_Market1501\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Táº¡o symlink tá»« local output sang Drive\n",
        "LOCAL_OUTPUT = \"/content/OpenPAR/PromptPAR/exp_result\"\n",
        "if os.path.exists(LOCAL_OUTPUT):\n",
        "    if os.path.islink(LOCAL_OUTPUT):\n",
        "        os.unlink(LOCAL_OUTPUT) # Remove symbolic link directly\n",
        "    elif os.path.isdir(LOCAL_OUTPUT):\n",
        "        import shutil\n",
        "        shutil.rmtree(LOCAL_OUTPUT) # Remove directory\n",
        "os.symlink(CHECKPOINT_DIR, LOCAL_OUTPUT)\n",
        "\n",
        "# Tá»± Ä‘á»™ng tÃ¬m checkpoint má»›i nháº¥t Ä‘á»ƒ resume\n",
        "checkpoint_files = glob.glob(f\"{CHECKPOINT_DIR}/*.pth\")\n",
        "resume_arg = \"\"\n",
        "if checkpoint_files:\n",
        "    latest_ckpt = max(checkpoint_files, key=os.path.getctime)\n",
        "    print(f\"TÃ¬m tháº¥y checkpoint má»›i nháº¥t â†’ tiáº¿p tá»¥c tá»«: {os.path.basename(latest_ckpt)}\")\n",
        "    resume_arg = f\"--checkpoint\"\n",
        "else:\n",
        "    print(\"KhÃ´ng cÃ³ checkpoint â†’ train tá»« Ä‘áº§u\")\n",
        "\n",
        "print(f\" Symlink: {LOCAL_OUTPUT} â†’ {CHECKPOINT_DIR}\")\n",
        "print(\" Báº¯t Ä‘áº§u training.. .\\n\")\n",
        "\n",
        "cmd = f\"\"\"\n",
        "python train.py Market1501 \\\n",
        "--batchsize 32 \\\n",
        "--epoch 60 \\\n",
        "--height 224 \\\n",
        "--width 224 \\\n",
        "--lr 5e-5 \\\n",
        "--weight_decay 1e-4 \\\n",
        "--clip_lr 1e-5 \\\n",
        "--clip_weight_decay 1e-4 \\\n",
        "--text_prompt 3 \\\n",
        "--vis_prompt 20 \\\n",
        "--vis_depth 12 \\\n",
        "--div_num 4 \\\n",
        "--overlap_row 2 \\\n",
        "--mm_layers 1 \\\n",
        "--smooth_param 0.1 \\\n",
        "--ag_threshold 0.5 \\\n",
        "--train_split trainval \\\n",
        "--valid_split test \\\n",
        "--save_freq 5 \\\n",
        "--use_div \\\n",
        "--use_textprompt \\\n",
        "--use_mm_former \\\n",
        "--use_GL \\\n",
        "--dir {CHECKPOINT_DIR} \\\n",
        "{resume_arg}\n",
        "\"\"\"\n",
        "\n",
        "print(cmd)\n",
        "!{cmd}\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}